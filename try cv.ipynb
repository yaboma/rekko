{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from implicit.nearest_neighbours import TFIDFRecommender\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from pprint import pprint\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'catalogue.json'), 'r') as f:\n",
    "    catalogue = json.load(f)\n",
    "    \n",
    "catalogue = {int(k): v for k, v in catalogue.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(catalogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(catalogue[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `attributes` — мешок атрибутов\n",
    " - `availability` — доступность (может содержать значения `purchase`, `rent` и `subscription`)\n",
    " - `duration` — длительность в минутах, округлённая до десятков (продолжительность серии для сериалов и многосерийных фильмов)\n",
    " - `feature_1..5` — пять анонимизированных вещественных и порядковых признаков\n",
    " - `type` — принимает значения `movie`, `multipart_movie` или `series`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_users.json` содержит список пользователей, для которых необходимо построить предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'test_users.json'), 'r') as f:\n",
    "    test_users = set(json.load(f)['users'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transactions.csv` — список всех транзакций за определённый период времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, 'transactions.csv'),\n",
    "    dtype={\n",
    "        'element_uid': np.uint16,\n",
    "        'user_uid': np.uint32,\n",
    "        'consumption_mode': 'category',\n",
    "        'ts': np.float64,\n",
    "        'watched_time': np.uint64,\n",
    "        'device_type': np.uint8,\n",
    "        'device_manufacturer': np.uint8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transactions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `element_uid` — идентификатор элемента\n",
    " - `user_uid` — идентификатор пользователя\n",
    " - `consumption_mode` — тип потребления (`P` — покупка, `R` — аренда, `S` — просмотр по подписке)\n",
    " - `ts` — время совершения транзакции или начала просмотра в случае просмотра по подписке\n",
    " - `watched_time` — число просмотренных по транзакции секунд\n",
    " - `device_type` — анонимизированный тип устройства, с которого была совершена транзакция или начат просмотр\n",
    " - `device_manufacturer` — анонимизированный производитель устройства, с которого была совершена транзакция или начат просмотр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ratings.csv` содержит информацию о поставленных пользователями оценках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, 'ratings.csv'),\n",
    "    dtype={\n",
    "        'element_uid': np.uint16,\n",
    "        'user_uid': np.uint32,\n",
    "        'ts': np.float64,\n",
    "        'rating': np.uint8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `rating` — поставленный пользователем рейтинг (от `0` до `10`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bookmarks.csv` содержит информацию об элементах, добавленных пользователями в список «Избранное»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, 'bookmarks.csv'),\n",
    "    dtype={\n",
    "        'element_uid': np.uint16,\n",
    "        'user_uid': np.uint32,\n",
    "        'ts': np.float64\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmarks.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала построим список элементов, которые тестовые пользователи уже купили или посмотрели по подписке: они не смогут купить их второй раз, а просмотр по подписке второй раз маловероятен, поэтому мы захотим отфильтровать такие элементы из финального ответа.\n",
    "\n",
    "Точно так же можно поступить и с рейтингами и добавлениями в избранное, если это будет казаться правильным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "filtered_elements = defaultdict(set)\n",
    "\n",
    "for user_uid, element_uid in tqdm_notebook(transactions.loc[:, ['user_uid', 'element_uid']].values):\n",
    "    if user_uid not in test_users:\n",
    "        continue\n",
    "    filtered_elements[user_uid].add(element_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера мы воспользуемся методом K ближайших соседей, реализованным в библиотеке `implicit`. В качестве данных используем только информацию о рейтингах.\n",
    "\n",
    "Необходимо построить разреженную матрицу, где строкам будут соответствовать элементы, столбцам — пользователи, а на пересечении пользователя и элемента будет находиться количественная оценка степени их взаимодействия, если таковое имело место.\n",
    "\n",
    "Не забудем добавить `1` к рейтингу, чтобы избежать деления на ноль во время вычисления `tf-idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['user_uid'] = ratings['user_uid'].astype('category')\n",
    "ratings['element_uid'] = ratings['element_uid'].astype('category')\n",
    "\n",
    "ratings_matrix = sp.coo_matrix(\n",
    "    (ratings['rating'].astype(np.float32) + 1,\n",
    "        (\n",
    "            ratings['element_uid'].cat.codes.copy(),\n",
    "            ratings['user_uid'].cat.codes.copy()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "ratings_matrix = ratings_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparsity = ratings_matrix.nnz / (ratings_matrix.shape[0] * ratings_matrix.shape[1])\n",
    "print('Sparsity: %.6f' % sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить модель крайне просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.nearest_neighbours import TFIDFRecommender\n",
    "\n",
    "model = TFIDFRecommender()\n",
    "model.fit(ratings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix_T = ratings_matrix.T.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отображения из оригинальной категории во внутреннюю пригодится нам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_uid_to_cat = dict(zip(\n",
    "    ratings['user_uid'].cat.categories,\n",
    "    range(len(ratings['user_uid'].cat.categories))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_uid_to_cat = dict(zip(\n",
    "    ratings['element_uid'].cat.categories,\n",
    "    range(len(ratings['element_uid'].cat.categories))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_elements_cat = {k: [element_uid_to_cat.get(x, None) for x in v] for k, v in filtered_elements.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В метод `model.recommend` мы передаём идентификатор пользователя, который получаем обратным преобразованием из категории, транспонированную матрицу взаимодействий, число необходимых рекомендаций и список элементов, которые мы договорились фильтровать из ответа.\n",
    "\n",
    "Возвращает метод список пар (`element_cat`, `score`), отсортированный по вторым элементам. Из него необходимо достать все первые элементы пар и из категории преобразовать их к `element_uid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно:** Не все тестовые пользователи есть в `ratings.csv` и не все из них есть в `transactions.csv`. Используя только один источник данных мы не можем построить полное предсказание. Такой ответ с неполным числом пользователей бдет принят системой, но при вычислении средней метрики метрика для отсутствующих пользователей будет принята равной нулю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "\n",
    "for user_uid in tqdm.tqdm(test_users):\n",
    "    # transform user_uid to model's internal user category\n",
    "    try:\n",
    "        user_cat = user_uid_to_cat[user_uid]\n",
    "    except LookupError:\n",
    "        continue\n",
    "    \n",
    "    # perform inference\n",
    "    recs = model.recommend(\n",
    "        user_cat,\n",
    "        ratings_matrix_T,\n",
    "        N=20,\n",
    "        filter_already_liked_items=True,\n",
    "        filter_items=filtered_elements_cat.get(user_uid, set())\n",
    "    )\n",
    "    \n",
    "    # drop scores and transform model's internal elelemnt category to element_uid for every prediction\n",
    "    # also convert np.uint64 to int so it could be json serialized later\n",
    "    result[user_uid] = [int(ratings['element_uid'].cat.categories[i]) for i, _ in recs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя только информацию о рейтингах мы смогли построить предсказание для `13251` из `50000` тестовых пользователей. Ровно в таком виде ответы и стоит сохранить для отправки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отсюда мое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, 'ratings.csv'),\n",
    "    dtype={\n",
    "        'element_uid': np.uint16,\n",
    "        'user_uid': np.uint32,\n",
    "        'ts': np.float64,\n",
    "        'rating': np.uint8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cv_split(n_splits, data_train): # old version\n",
    "    \"\"\"\n",
    "    Разделяем данные на трейн юзеров и тест юзеров по уникальным пользователям\n",
    "    :n_splits: кол-во фолдов\n",
    "    :data_train: данные для обучения\n",
    "    \"\"\"\n",
    "    unique_users = np.unique(data_train['user_uid'])\n",
    "    split_data = np.array_split(unique_users, n_splits)\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        train_users, test_users = np.hstack([x for i,x in enumerate(split_data) if i != fold]), split_data[fold]\n",
    "        yield train_users, test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split(n_splits, data_train): # new version\n",
    "    \"\"\"\n",
    "    Разделяем данные на трейн юзеров и тест юзеров по уникальным пользователям\n",
    "    :n_splits: кол-во фолдов\n",
    "    :data_train: данные для обучения\n",
    "    \"\"\"\n",
    "    unique_users = np.unique(data_train['user_uid'])\n",
    "    split_data = np.array_split(unique_users, n_splits)\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        test_users = split_data[fold]\n",
    "        train_users = np.setdiff1d(unique_users, test_users)\n",
    "        yield train_users, test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dfs(data_train, count, *cv_fold, data=transactions): # old version\n",
    "    \"\"\"\n",
    "    Оставляем в тесте и трейне только тех у кого более или равно count кол-во фильмов,\n",
    "    выдаем данные для обучения и оценки\n",
    "    :data_train: данные для обучения\n",
    "    :count: кол-во фильмов, для трейна и предикта, также это кол-во фильмов для предсказаний\n",
    "    :data: данные для выбора только тех юзеров у которых есть определенное кол-во фильмов\n",
    "    true_data - словарь с релевантным 20 (count) кол-вом фильмов, для теста, с которым будет сверятся на метрике\n",
    "    \"\"\"\n",
    "    data = data[data.groupby('user_uid')['element_uid'].transform('size') >= count]\n",
    "    unique_users = np.unique(data['user_uid'])\n",
    "    \n",
    "    data.sort_values(by=['user_uid', 'watched_time'], ascending=False, inplace=True)\n",
    "    data = data.groupby('user_uid').head(count) ## ВОТ ЭТО ДЕЙСТВИЕ НАДО ДЕЛАТЬ ИЛИ НЕТ?\n",
    "    data = data.groupby('user_uid').element_uid.apply(list).to_dict()\n",
    "      \n",
    "    train_users, test_users = cv_fold[0], cv_fold[1]\n",
    "    train_users = np.intersect1d(train_users, unique_users)\n",
    "    test_users = np.intersect1d(test_users, unique_users)\n",
    "    train_df = data_train[data_train['user_uid'].isin(unique_users)]\n",
    "    test_df = data_train[data_train['user_uid'].isin(unique_users)]\n",
    "    \n",
    "    true_data = {k:v for k,v in data.items() if k in test_users}\n",
    "    \n",
    "    return train_df, test_df, true_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfs(data_train, count, *cv_fold, data=transactions): # new version\n",
    "    \"\"\"\n",
    "    Оставляем в тесте и трейне только тех у кого более или равно count кол-во фильмов,\n",
    "    выдаем данные для обучения и оценки\n",
    "    :data_train: данные для обучения\n",
    "    :count: кол-во фильмов, для трейна и предикта, также это кол-во фильмов для предсказаний\n",
    "    :data: данные для выбора только тех юзеров у которых есть определенное кол-во фильмов\n",
    "    true_data - словарь с релевантным 20 (count) кол-вом фильмов, для теста, с которым будет сверятся на метрике\n",
    "    \"\"\"\n",
    "    unique_users = np.unique(data['user_uid'])\n",
    "    \n",
    "    data.sort_values(by=['user_uid', 'watched_time'], ascending=False, inplace=True)\n",
    "    data = data.groupby('user_uid').element_uid.apply(list).to_dict()\n",
    "      \n",
    "    train_users, test_users = cv_fold[0], cv_fold[1]\n",
    "\n",
    "    train_df = data_train[data_train['user_uid'].isin(unique_users)]\n",
    "    test_df = data_train[data_train['user_uid'].isin(unique_users)]\n",
    "    \n",
    "    true_data = {k:v for k,v in data.items() if k in test_users}\n",
    "    \n",
    "    return train_df, test_df, true_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_df(train_df, test_df):   \n",
    "    \"\"\"\n",
    "    Соединяем train_df и test_df воедино (вот этот момент мне не ясен, но вроде так надо). \n",
    "    Получаем разреженную матрицу от всего общего df, траспонированную, user_uid_to_cat, df\n",
    "    \"\"\"\n",
    "    df = train_df.append(test_df).sort_index()\n",
    "    df['user_uid'] = df['user_uid'].astype('category')\n",
    "    df['element_uid'] = df['element_uid'].astype('category')\n",
    "\n",
    "    ratings_matrix = sp.coo_matrix(\n",
    "        (df['rating'].astype(np.float32) + 1,\n",
    "            (\n",
    "                df['element_uid'].cat.codes.copy(),\n",
    "                df['user_uid'].cat.codes.copy()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    user_uid_to_cat = dict(zip(\n",
    "        df['user_uid'].cat.categories,\n",
    "        range(len(df['user_uid'].cat.categories))\n",
    "    ))\n",
    "\n",
    "    ratings_matrix = ratings_matrix.tocsr()\n",
    "    ratings_matrix_T = ratings_matrix.T.tocsr()\n",
    "    \n",
    "    return ratings_matrix, ratings_matrix_T, user_uid_to_cat, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(ratings_matrix):\n",
    "    \"\"\" \n",
    "    Обучение модели на разреженной матрице\n",
    "    \"\"\"\n",
    "    model = TFIDFRecommender()\n",
    "    model.fit(ratings_matrix)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_predicted_data(test_df, user_uid_to_cat, ratings_matrix_T, df, count): # old version\n",
    "    \"\"\"\n",
    "    Получаем предсказзанный словарь с count - необходимым кол-вом фильмов\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_data = {}\n",
    "\n",
    "    for user_uid in tqdm.tqdm(test_df.user_uid.values):\n",
    "        try:\n",
    "            user_cat = user_uid_to_cat[user_uid]\n",
    "        except LookupError:\n",
    "            continue\n",
    "\n",
    "        recs = model.recommend(\n",
    "            user_cat,\n",
    "            ratings_matrix_T,\n",
    "            N=count,)\n",
    "    #         filter_already_liked_items=True,\n",
    "    #         filter_items=filtered_elements_cat.get(user_uid, set())\n",
    "    #     )\n",
    "\n",
    "        predicted_data[user_uid] = [int(df['element_uid'].cat.categories[i]) for i, _ in recs]\n",
    "        \n",
    "    return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_data(test_df, user_uid_to_cat, ratings_matrix_T, df, count): # new version\n",
    "    \"\"\"\n",
    "    Получаем предсказанный словарь с count - необходимым кол-вом фильмов\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_data = {}\n",
    "    movies_top = transactions['element_uid'].value_counts().head(count).index.tolist()\n",
    "    \n",
    "    for user_uid in tqdm.tqdm(test_df.user_uid.values):\n",
    "        try:\n",
    "            user_cat = user_uid_to_cat[user_uid]\n",
    "            \n",
    "            recs = model.recommend(\n",
    "                user_cat,\n",
    "                ratings_matrix_T,\n",
    "                N=count)\n",
    "            \n",
    "            predicted_data[user_uid] = [int(df['element_uid'].cat.categories[i]) for i, _ in recs]    \n",
    "            \n",
    "        except LookupError:\n",
    "            predicted_data[user_uid] = movies_top\n",
    "        \n",
    "    return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скопировал код с https://habr.com/ru/company/okko/blog/439180/ хз правильно ли отредактировал с CPython\n",
    "def average_precision(data_true, data_predicted, k) -> float:\n",
    "\n",
    "    if not data_true:\n",
    "        raise ValueError('data_true is empty')\n",
    "\n",
    "    average_precision_sum = 0.0\n",
    "\n",
    "    for key, items_true in data_true.items():\n",
    "        items_predicted = data_predicted.get(key, [])\n",
    "\n",
    "        n_items_true = len(items_true)\n",
    "        n_items_predicted = min(len(items_predicted), k)\n",
    "\n",
    "        if n_items_true == 0 or n_items_predicted == 0:\n",
    "            continue\n",
    "\n",
    "        n_correct_items = 0\n",
    "        precision = 0.0\n",
    "\n",
    "        for item_idx in range(n_items_predicted):\n",
    "            if items_predicted[item_idx] in items_true:\n",
    "                n_correct_items += 1\n",
    "                precision += n_correct_items / (item_idx + 1)\n",
    "\n",
    "        average_precision_sum += precision / min(n_items_true, k)\n",
    "\n",
    "    return average_precision_sum / len(data_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(true_data, predicted_data, k):\n",
    "    true_data_set = {k: set(v) for k, v in true_data.items()}\n",
    "\n",
    "    return average_precision(true_data_set, predicted_data, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По новым методам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7468/7468 [00:00<00:00, 26695.34it/s]\n",
      "100%|██████████| 435189/435189 [01:23<00:00, 5222.97it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 121741.30it/s]\n",
      "100%|██████████| 435189/435189 [01:26<00:00, 5055.22it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 125033.68it/s]\n",
      "100%|██████████| 435189/435189 [01:10<00:00, 6183.54it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 94500.85it/s]\n",
      "100%|██████████| 435189/435189 [01:12<00:00, 5967.18it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 91297.19it/s]\n",
      "100%|██████████| 435189/435189 [01:13<00:00, 5925.40it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 93902.80it/s]\n",
      "100%|██████████| 435189/435189 [01:15<00:00, 5801.32it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 101589.08it/s]\n",
      "100%|██████████| 435189/435189 [01:13<00:00, 5942.71it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 93105.36it/s]\n",
      "100%|██████████| 435189/435189 [01:11<00:00, 6067.73it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 103539.78it/s]\n",
      "100%|██████████| 435189/435189 [01:11<00:00, 6093.78it/s]\n",
      "100%|██████████| 7468/7468 [00:00<00:00, 97943.95it/s]\n",
      "100%|██████████| 435189/435189 [01:13<00:00, 5887.90it/s]\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "k_list = [10, 20]\n",
    "count = 20\n",
    "metrics = {}\n",
    "for k in k_list:\n",
    "    counter = cv_split(n_splits, ratings)\n",
    "    temp_metrics = np.zeros(n_splits)\n",
    "    i = 0\n",
    "    for it in counter:\n",
    "        train_df, test_df, true_data = get_dfs(ratings, count, *it)\n",
    "        ratings_matrix, ratings_matrix_T, user_uid_to_cat, df = sparse_df(train_df, test_df)\n",
    "        model = model_fit(ratings_matrix)\n",
    "        predicted_data = get_predicted_data(test_df, user_uid_to_cat, ratings_matrix_T, df, count)\n",
    "        temp_metrics[i] = metric(true_data, predicted_data, k)\n",
    "        i += 1\n",
    "    metrics[k] = temp_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: array([0.05862126, 0.05793591, 0.0570175 , 0.05768103, 0.0579142 ]),\n",
       " 20: array([0.04925117, 0.04822313, 0.04775214, 0.04845391, 0.04833178])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По старым методам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7145/7145 [00:00<00:00, 116324.84it/s]\n",
      "100%|██████████| 305839/305839 [00:51<00:00, 5961.23it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 123455.39it/s]\n",
      "100%|██████████| 305839/305839 [00:52<00:00, 5790.86it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 91905.60it/s]\n",
      "100%|██████████| 305839/305839 [00:53<00:00, 5703.96it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 99850.07it/s]\n",
      "100%|██████████| 305839/305839 [00:52<00:00, 5803.44it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 104768.15it/s]\n",
      "100%|██████████| 305839/305839 [00:51<00:00, 5954.67it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 100146.71it/s]\n",
      "100%|██████████| 305839/305839 [00:52<00:00, 5823.88it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 143077.52it/s]\n",
      "100%|██████████| 305839/305839 [00:56<00:00, 5418.79it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 103206.94it/s]\n",
      "100%|██████████| 305839/305839 [00:56<00:00, 5451.69it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 110983.44it/s]\n",
      "100%|██████████| 305839/305839 [00:57<00:00, 5353.49it/s]\n",
      "100%|██████████| 7145/7145 [00:00<00:00, 81965.26it/s]\n",
      "100%|██████████| 305839/305839 [00:55<00:00, 5557.48it/s]\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "k_list = [10, 20]\n",
    "count = 20\n",
    "metrics = {}\n",
    "for k in k_list:\n",
    "    counter = _cv_split(n_splits, ratings)\n",
    "    temp_metrics = np.zeros(n_splits)\n",
    "    i = 0\n",
    "    for it in counter:\n",
    "        train_df, test_df, true_data = _get_dfs(ratings, count, *it)\n",
    "        ratings_matrix, ratings_matrix_T, user_uid_to_cat, df = sparse_df(train_df, test_df)\n",
    "        model = model_fit(ratings_matrix)\n",
    "        predicted_data = _get_predicted_data(test_df, user_uid_to_cat, ratings_matrix_T, df, count)\n",
    "        temp_metrics[i] = metric(true_data, predicted_data, k)\n",
    "        i += 1\n",
    "    metrics[k] = temp_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: array([0.0458047 , 0.04547763, 0.04599957, 0.04604166, 0.04529647]),\n",
       " 20: array([0.0287629 , 0.02851868, 0.02888257, 0.02894626, 0.02833982])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rekko)",
   "language": "python",
   "name": "rekko_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
